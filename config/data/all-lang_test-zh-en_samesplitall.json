{
    "other": "same_split",
    "datasets": ["coco", "flickr", "conceptual"],
    "training": [""],
    "testing": ["af", "az", "bs", "de", "fa-AF", "ha", "hr", "it", "ko", "ms", "pl", "ro", "sl", "sr", "ta", "tr",
           "vi", "am", "bg", "cs", "el", "et", "fi", "he", "hu", "ja", "nl", "ps", "ru", "so", "sv", "th", "uk",
           "ar", "bn", "da", "fa", "fr", "hi", "id", "ka", "lv", "no", "pt", "sk", "sq", "es", "sw", "tl", "ur"],
  "tokenizer_from": "all-lang_test-zh-en",
  "word2vec_from": ["all-lang_test-zh-en", "training"]
}